{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Laptop\\\\Downloads\\\\spark-2.4.7-bin-hadoop2.7\\\\spark-2.4.7-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, BooleanType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = StructType([\n",
    "    StructField(name='title', dataType=StringType(), nullable=True),\n",
    "    StructField(name='telecommuting', dataType=StringType(), nullable=True),\n",
    "    StructField(name='has_company_logo', dataType=StringType(), nullable=True),\n",
    "    StructField(name='has_questions', dataType=StringType(), nullable=True),\n",
    "    StructField(name='fraudulent', dataType=StringType(), nullable=True),\n",
    "    StructField(name='company_id', dataType=DoubleType(), nullable=False),        \n",
    "    StructField(name='full_description', dataType=StringType(), nullable=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(new_schema) \\\n",
    "    .options(path=\"DataSet\", header=True,delimiter = \";\",multiline = True) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- telecommuting: string (nullable = true)\n",
      " |-- has_company_logo: string (nullable = true)\n",
      " |-- has_questions: string (nullable = true)\n",
      " |-- fraudulent: string (nullable = true)\n",
      " |-- company_id: double (nullable = true)\n",
      " |-- full_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------------+-------------+----------+----------+--------------------+\n",
      "|               title|telecommuting|has_company_logo|has_questions|fraudulent|company_id|    full_description|\n",
      "+--------------------+-------------+----------------+-------------+----------+----------+--------------------+\n",
      "|    Marketing Intern|            f|               t|            f|         f|      45.0|US, NY, New York ...|\n",
      "|Customer Service ...|            f|               t|            f|         f|     104.0|NZ, , Auckland Su...|\n",
      "|Commissioning Mac...|            f|               t|            f|         f|       6.0|US, IA, Wever  <h...|\n",
      "|Account Executive...|            f|               t|            f|         f|      37.0|US, DC, Washingto...|\n",
      "| Bill Review Manager|            f|               t|            t|         f|      25.0|US, FL, Fort Wort...|\n",
      "|    Accounting Clerk|            f|               f|            f|         f|       0.0|US, MD,    <p><b>...|\n",
      "|Head of Content (...|            f|               t|            t|         f|     821.0|DE, BE, Berlin AN...|\n",
      "|Lead Guest Servic...|            f|               t|            t|         f|     173.0|US, CA, San Franc...|\n",
      "|          HP BSM SME|            f|               t|            t|         f|     608.0|US, FL, Pensacola...|\n",
      "|Customer Service ...|            f|               t|            f|         f|       3.0|US, AZ, Phoenix  ...|\n",
      "|ASP.net Developer...|            f|               f|            f|         f|       0.0|US, NJ, Jersey Ci...|\n",
      "|Talent Sourcer (6...|            f|               t|            f|         f|       9.0|GB, LND, London H...|\n",
      "|Applications Deve...|            f|               t|            f|         f|       3.0|US, CT, Stamford ...|\n",
      "|          Installers|            f|               t|            t|         f|     206.0|US, FL, Orlando  ...|\n",
      "|Account Executive...|            f|               t|            f|         f|     134.0|AU, NSW, Sydney S...|\n",
      "|VP of Sales - Vau...|            f|               t|            t|         f|     659.0|SG, 01, Singapore...|\n",
      "|  Hands-On QA Leader|            f|               t|            f|         f|     197.0|IL, , Tel Aviv, I...|\n",
      "|Southend-on-Sea T...|            f|               t|            t|         f|       4.0|GB, SOS, Southend...|\n",
      "|     Visual Designer|            f|               t|            f|         f|      50.0|US, NY, New York ...|\n",
      "|Process Controls ...|            f|               f|            f|         f|       2.0|US, PA, USA North...|\n",
      "+--------------------+-------------+----------------+-------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler,HashingTF, Tokenizer,IDF\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = new_data['telecommuting','has_company_logo','fraudulent'].randomSplit([0.7, 0.3])\n",
    "\n",
    "categoricalColumns = ['telecommuting', 'has_company_logo']\n",
    "\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    stages += [stringIndexer]\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"fraudulent\", outputCol=\"label\")\n",
    "\n",
    "assemblerInputs = [c + \"Index\" for c in categoricalColumns]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "stages += [labelIndexer,assembler, rf]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "predictions = model.transform(testData)\n",
    "predictions.select(\"label\", \"features\").show(5)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName=\"areaUnderPR\")\n",
    "areaUnderPR = evaluator.evaluate(predictions)\n",
    "print(\"areaUnderPR = %g\" % (areaUnderPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = new_data['title','fraudulent'].randomSplit([0.7, 0.3])\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"fraudulent\", outputCol=\"label\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "pipeline = Pipeline(stages=[labelIndexer,tokenizer, hashingTF, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(rf.maxDepth, [2, 4,6]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)  \n",
    "\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "prediction = cvModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName=\"areaUnderPR\")\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = new_data['full_description','fraudulent'].randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"fraudulent\", outputCol=\"label\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"full_description\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[labelIndexer,tokenizer, hashingTF, idf,rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10,15]) \\\n",
    "    .addGrid(rf.numTrees, [10, 100,1000]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "prediction = cvModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName=\"areaUnderPR\")\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
